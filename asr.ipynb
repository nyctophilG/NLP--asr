{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6f1afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "    HubertForCTC, \n",
    "    WhisperProcessor, WhisperForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b969aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Real German Data (flozi00/asr-german-mixed-evals)...\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Loading Real German Data (flozi00/asr-german-mixed-evals)...\")\n",
    "\n",
    "# Load dataset (streaming mode)\n",
    "dataset_stream = load_dataset(\n",
    "    \"flozi00/asr-german-mixed-evals\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "105c4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sample():\n",
    "    sample = next(iter(dataset_stream))\n",
    "    \n",
    "    # 1. Get the raw list of numbers\n",
    "    audio_data = sample[\"audio\"][\"array\"]\n",
    "    orig_sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # 2. CRITICAL FIX: Convert List -> NumPy Array -> Tensor\n",
    "    # The error happened because 'audio_data' was a list. \n",
    "    # np.array() fixes it.\n",
    "    audio_tensor = torch.from_numpy(np.array(audio_data)).float()\n",
    "    \n",
    "    # 3. Manual Resampling to 16000 Hz (Standard for these models)\n",
    "    if orig_sr != 16000:\n",
    "        resampler = T.Resample(orig_sr, 16000)\n",
    "        audio_tensor = resampler(audio_tensor)\n",
    "    \n",
    "    # 4. Extract Text\n",
    "    text = sample[\"references\"]\n",
    "    \n",
    "    return audio_tensor, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3251f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data Loaded Successfully.\n",
      "   Sample Text: Sie hätten jedenfalls sogleich die sicherste Kontrolle für meine Darstellung an ihr, auf der anderen Seite gewinnt aber diese vielleicht an Unbefangenheit und historischer Treue.\n",
      "   Audio Shape: torch.Size([193280])\n"
     ]
    }
   ],
   "source": [
    "audio_check, text_check = get_next_sample()\n",
    "print(f\"   Data Loaded Successfully.\")\n",
    "print(f\"   Sample Text: {text_check}\")\n",
    "print(f\"   Audio Shape: {audio_check.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2e95a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(64, n_classes, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # FIX: Handle 1D input [Time] -> 3D [1, 1, Time]\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "        # FIX: Handle 2D input [Batch, Time] -> 3D [Batch, 1, Time]\n",
    "        elif x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        return self.cnn(x).permute(0, 2, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44347351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_ctc_decode(logits, vocab):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    best_path = torch.argmax(probs, dim=-1)[0]\n",
    "    \n",
    "    decoded_chars = []\n",
    "    prev_idx = -1\n",
    "    \n",
    "    for idx in best_path:\n",
    "        idx = idx.item()\n",
    "        if idx != prev_idx and idx != 0:\n",
    "            char = vocab.get(idx, \"\")\n",
    "            decoded_chars.append(char)\n",
    "        prev_idx = idx\n",
    "        \n",
    "    return \"\".join(decoded_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c28cd935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading HuBERT (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading HuBERT (German)...\")\n",
    "processor_3 = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model_3 = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d83fcf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Whisper (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading Whisper (German)...\")\n",
    "processor_4 = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model_4 = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fdf2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Wav2Vec2 XLS-R (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading Wav2Vec2 XLS-R (German)...\")\n",
    "w2v_id = \"facebook/wav2vec2-large-xlsr-53-german\"\n",
    "processor_5 = Wav2Vec2Processor.from_pretrained(w2v_id)\n",
    "model_5 = Wav2Vec2ForCTC.from_pretrained(w2v_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c38e8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        output, _ = self.rnn(x)\n",
    "        return self.fc(output)\n",
    "    \n",
    "model_6 = SimpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13801c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "    HubertForCTC, \n",
    "    WhisperProcessor, WhisperForConditionalGeneration\n",
    ")\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "daa5e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_ctc_decode(logits, processor):\n",
    "    # Rule-based decoding (Greedy Search) without using the library's .decode()\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f96e0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, n_classes, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FIX 1: If input is just 1D audio [Time], make it [1, 1, Time]\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "        # FIX 2: If input is 2D batch [Batch, Time], make it [Batch, 1, Time]\n",
    "        elif x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        return self.cnn(x).permute(0, 2, 1)\n",
    "\n",
    "model_1 = BaselineCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3456022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, n_classes)\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1: x = x.unsqueeze(0).unsqueeze(2) \n",
    "        x, _ = self.rnn(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "99f44a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Pre-trained Brains (HuBERT, Whisper, Wav2Vec2)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading Pre-trained Brains (HuBERT, Whisper, Wav2Vec2)...\")\n",
    "# HuBERT\n",
    "p_hubert = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "m_hubert = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e570523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper\n",
    "p_whisper = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "m_whisper = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8262251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav2Vec2 XLS-R (German)\n",
    "p_w2v = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-german\")\n",
    "m_w2v = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c2add00",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ground_truth = text_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "82863e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need to check if it runs. Output will be garbage.\n",
    "cnn = BaselineCNN()\n",
    "with torch.no_grad():\n",
    "    out = cnn(audio_check)\n",
    "    pred_1 = \"random_init_output\" # Placeholder as it's untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Manual CTC (Applied to Wav2Vec2 Logits)\n",
    "with torch.no_grad():\n",
    "    logits = m_w2v(audio_check.unsqueeze(0)).logits\n",
    "    pred_2 = manual_ctc_decode(logits, p_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44f9f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HuBERT\n",
    "with torch.no_grad():\n",
    "    logits = m_hubert(audio_check.unsqueeze(0)).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_3 = p_hubert.batch_decode(pred_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b89c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# 4. Whisper\n",
    "with torch.no_grad():\n",
    "    input_features = p_whisper(audio_check, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    gen_ids = m_whisper.generate(input_features)\n",
    "    pred_4 = p_whisper.batch_decode(gen_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "44066c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Wav2Vec2 XLS-R\n",
    "with torch.no_grad():\n",
    "    logits = m_w2v(audio_check.unsqueeze(0)).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_5 = p_w2v.batch_decode(pred_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8740582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. RNN (Random)\n",
    "rnn = SimpleRNN()\n",
    "with torch.no_grad():\n",
    "    out = rnn(audio_check)\n",
    "    pred_6 = \"random_init_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5fdcfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL PROJECT REPORT (German Sample)\n",
      "============================================================\n",
      "GROUND TRUTH: \"Sie hätten jedenfalls sogleich die sicherste Kontrolle für meine Darstellung an ihr, auf der anderen Seite gewinnt aber diese vielleicht an Unbefangenheit und historischer Treue.\"\n",
      "------------------------------------------------------------\n",
      "METHOD                    | WER        | TRANSCRIPTION (First 50 chars)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL PROJECT REPORT (German Sample)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GROUND TRUTH: \\\"{ground_truth}\\\"\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'METHOD':<25} | {'WER':<10} | {'TRANSCRIPTION (First 50 chars)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "predictions = [\n",
    "    (\"1. Baseline CNN\", pred_1),\n",
    "    (\"2. Manual CTC (Rule)\", pred_2),\n",
    "    (\"3. HuBERT (Transf)\", pred_3),\n",
    "    (\"4. Whisper (Enc-Dec)\", pred_4),\n",
    "    (\"5. Wav2Vec2 (XLS-R)\", pred_5),\n",
    "    (\"6. Simple RNN\", pred_6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dce2918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Baseline CNN           | 1.0000     | random_init_output...\n",
      "2. Manual CTC (Rule)      | 0.2800     | sie hätten jedenfalls sogleich die sicherste kontr...\n",
      "3. HuBERT (Transf)        | 1.1200     | ZHE HADN YEDEN FIL SOGLAICH DI ZE HESTO CONTROLLEF...\n",
      "4. Whisper (Enc-Dec)      | 0.1200     |  Sie hätten jedenfalls zugleich die sicherste Kont...\n",
      "5. Wav2Vec2 (XLS-R)       | 0.2800     | sie hätten jedenfalls sogleich die sicherste kontr...\n",
      "6. Simple RNN             | 1.0000     | random_init_output...\n"
     ]
    }
   ],
   "source": [
    "for name, pred in predictions:\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    # If pred is garbage, WER is high (1.0 or more)\n",
    "    if \"random\" in pred:\n",
    "        error_rate = 1.0\n",
    "    else:\n",
    "        error_rate = wer(ground_truth, pred)\n",
    "        \n",
    "    print(f\"{name:<25} | {error_rate:.4f}     | {pred[:50]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
