{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6f1afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "    HubertForCTC, \n",
    "    WhisperProcessor, WhisperForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b969aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Real German Data (flozi00/asr-german-mixed-evals)...\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Loading Real German Data (flozi00/asr-german-mixed-evals)...\")\n",
    "\n",
    "# Load dataset (streaming mode)\n",
    "dataset_stream = load_dataset(\n",
    "    \"flozi00/asr-german-mixed-evals\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "105c4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sample():\n",
    "    sample = next(iter(dataset_stream))\n",
    "    \n",
    "    # 1. Get the raw list of numbers\n",
    "    audio_data = sample[\"audio\"][\"array\"]\n",
    "    orig_sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    # 2. CRITICAL FIX: Convert List -> NumPy Array -> Tensor\n",
    "    # The error happened because 'audio_data' was a list. \n",
    "    # np.array() fixes it.\n",
    "    audio_tensor = torch.from_numpy(np.array(audio_data)).float()\n",
    "    \n",
    "    # 3. Manual Resampling to 16000 Hz (Standard for these models)\n",
    "    if orig_sr != 16000:\n",
    "        resampler = T.Resample(orig_sr, 16000)\n",
    "        audio_tensor = resampler(audio_tensor)\n",
    "    \n",
    "    # 4. Extract Text\n",
    "    text = sample[\"references\"]\n",
    "    \n",
    "    return audio_tensor, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3251f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data Loaded Successfully.\n",
      "   Sample Text: Sie hätten jedenfalls sogleich die sicherste Kontrolle für meine Darstellung an ihr, auf der anderen Seite gewinnt aber diese vielleicht an Unbefangenheit und historischer Treue.\n",
      "   Audio Shape: torch.Size([193280])\n"
     ]
    }
   ],
   "source": [
    "audio_check, text_check = get_next_sample()\n",
    "print(f\"   Data Loaded Successfully.\")\n",
    "print(f\"   Sample Text: {text_check}\")\n",
    "print(f\"   Audio Shape: {audio_check.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2e95a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, n_classes, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        return self.cnn(x).permute(0, 2, 1)\n",
    "\n",
    "model_1 = BaselineCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44347351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_ctc_decode(logits, vocab):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    best_path = torch.argmax(probs, dim=-1)[0]\n",
    "    \n",
    "    decoded_chars = []\n",
    "    prev_idx = -1\n",
    "    \n",
    "    for idx in best_path:\n",
    "        idx = idx.item()\n",
    "        if idx != prev_idx and idx != 0:\n",
    "            char = vocab.get(idx, \"\")\n",
    "            decoded_chars.append(char)\n",
    "        prev_idx = idx\n",
    "        \n",
    "    return \"\".join(decoded_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28cd935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading HuBERT (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading HuBERT (German)...\")\n",
    "processor_3 = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model_3 = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d83fcf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Whisper (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading Whisper (German)...\")\n",
    "processor_4 = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model_4 = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fdf2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Wav2Vec2 XLS-R (German)...\n"
     ]
    }
   ],
   "source": [
    "print(\"   Loading Wav2Vec2 XLS-R (German)...\")\n",
    "w2v_id = \"facebook/wav2vec2-large-xlsr-53-german\"\n",
    "processor_5 = Wav2Vec2Processor.from_pretrained(w2v_id)\n",
    "model_5 = Wav2Vec2ForCTC.from_pretrained(w2v_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c38e8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, n_classes=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        output, _ = self.rnn(x)\n",
    "        return self.fc(output)\n",
    "    \n",
    "model_6 = SimpleRNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
