# NLP--asr
Automated Speech Recognition: From Baselines to Transformer Architectures
This repository features an extensive technical evaluation of Automated Speech Recognition (ASR) systems, specifically applied to German language audio. The project is designed as a comparative benchmark that tracks the performance of diverse architectural paradigms, starting from scratch-built neural networks and advancing to high-capacity, pre-trained transformer models. By utilizing a unified dataset (flozi00/asr-german-mixed-evals) and standardizing audio preprocessing to a 16000 Hz sampling rate, the repository provides a controlled environment to measure the Word Error Rate (WER) across different modeling strategies.

The initial phase of the project explores foundational deep learning structures, including a custom Baseline Convolutional Neural Network (CNN) and a Bidirectional Gated Recurrent Unit (GRU) based Simple RNN. These models represent the structural building blocks of audio processing, focusing on extracting local acoustic features and modeling temporal dependencies respectively. While these baselines establish the architectural logic for handling 1D audio signals, the project demonstrates the limitations of training such models from scratch on limited data compared to robust, large-scale solutions.

A significant portion of the repository is dedicated to state-of-the-art sequence-to-sequence and Connectionist Temporal Classification (CTC) based models. This includes a manual implementation of a CTC decoding rule applied to Wav2Vec2 logits, which showcases the mathematical process of collapsing frame-level predictions into coherent text. The project evaluates several prominent architectures, including Facebook's HuBERT and Wav2Vec2 XLS-R—the latter being specifically tuned for German—as well as OpenAI’s Whisper, an encoder-decoder transformer. This comparison highlights the differences between self-supervised representations and fully supervised generative models in decoding complex phonetic structures.

The final output is a comprehensive performance report that benchmarks each method using the Word Error Rate metric. By analyzing the transcriptions produced by each model—ranging from the raw, unoptimized outputs of the CNN to the highly accurate, context-aware transcriptions of the Whisper model—this project provides a clear technical roadmap of modern speech-to-text evolution. The inclusion of manual decoding scripts alongside high-level library abstractions makes this repository a valuable resource for understanding the internal mechanics of how audio waveforms are translated into human language.
